{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4853.67it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)\n",
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor = Dense(1, activation = \"relu\")\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "\n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "\n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "\n",
    "    s_prev_repeated = repeator(s_prev)\n",
    "    concat = concatenator([s_prev_repeated, a])\n",
    "    # e_pre = densor2(densor1(concat))  # Why to use two densor?\n",
    "    e = densor(concat)\n",
    "    alpha = activator(e)\n",
    "    context = dotor([alpha, a])\n",
    "\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_a = 64 # 32\n",
    "n_s = 128 # 64\n",
    "# post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "# output_layer = Dense(len(machine_vocab), activation=softmax)\n",
    "\n",
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    Y = []\n",
    "\n",
    "    pre_LSTM_layer = Bidirectional(LSTM(units=n_a, return_sequences=True))\n",
    "    post_LSTM_cell = LSTM(n_s, return_state=True)\n",
    "    output_densor = Dense(machine_vocab_size, activation=softmax)\n",
    "\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    a = pre_LSTM_layer(X)\n",
    "\n",
    "    for t in range(Ty):\n",
    "        context = one_step_attention(a, s)\n",
    "        s, _, c = post_LSTM_cell(context, initial_state=[s, c])\n",
    "        y = output_densor(s)\n",
    "        Y.append(y)\n",
    "\n",
    "    return Model(input=[X, s0, c0], output=Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "s0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 30, 37)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 30, 128)      0           s0[0][0]                         \n",
      "                                                                 lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "                                                                 lstm_2[2][0]                     \n",
      "                                                                 lstm_2[3][0]                     \n",
      "                                                                 lstm_2[4][0]                     \n",
      "                                                                 lstm_2[5][0]                     \n",
      "                                                                 lstm_2[6][0]                     \n",
      "                                                                 lstm_2[7][0]                     \n",
      "                                                                 lstm_2[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30, 128)      52224       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 256)      0           repeat_vector_1[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[4][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[5][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[6][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[7][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[8][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[9][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30, 1)        257         concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "                                                                 concatenate_1[4][0]              \n",
      "                                                                 concatenate_1[5][0]              \n",
      "                                                                 concatenate_1[6][0]              \n",
      "                                                                 concatenate_1[7][0]              \n",
      "                                                                 concatenate_1[8][0]              \n",
      "                                                                 concatenate_1[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 128)       0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 128), (None, 131584      dot_1[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_2[1][0]                     \n",
      "                                                                 lstm_2[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_2[2][0]                     \n",
      "                                                                 lstm_2[2][2]                     \n",
      "                                                                 dot_1[4][0]                      \n",
      "                                                                 lstm_2[3][0]                     \n",
      "                                                                 lstm_2[3][2]                     \n",
      "                                                                 dot_1[5][0]                      \n",
      "                                                                 lstm_2[4][0]                     \n",
      "                                                                 lstm_2[4][2]                     \n",
      "                                                                 dot_1[6][0]                      \n",
      "                                                                 lstm_2[5][0]                     \n",
      "                                                                 lstm_2[5][2]                     \n",
      "                                                                 dot_1[7][0]                      \n",
      "                                                                 lstm_2[6][0]                     \n",
      "                                                                 lstm_2[6][2]                     \n",
      "                                                                 dot_1[8][0]                      \n",
      "                                                                 lstm_2[7][0]                     \n",
      "                                                                 lstm_2[7][2]                     \n",
      "                                                                 dot_1[9][0]                      \n",
      "                                                                 lstm_2[8][0]                     \n",
      "                                                                 lstm_2[8][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 11)           1419        lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "                                                                 lstm_2[2][0]                     \n",
      "                                                                 lstm_2[3][0]                     \n",
      "                                                                 lstm_2[4][0]                     \n",
      "                                                                 lstm_2[5][0]                     \n",
      "                                                                 lstm_2[6][0]                     \n",
      "                                                                 lstm_2[7][0]                     \n",
      "                                                                 lstm_2[8][0]                     \n",
      "                                                                 lstm_2[9][0]                     \n",
      "==================================================================================================\n",
      "Total params: 185,484\n",
      "Trainable params: 185,484\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model\n",
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train this Model\n",
    "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01) # lr = learning_rate\n",
    "model.compile(optimizer=opt ,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/55\n",
      "10000/10000 [==============================] - 55s 6ms/step - loss: 14.8478 - dense_4_loss: 2.4908 - dense_4_acc: 0.5597 - dense_4_acc_1: 0.7554 - dense_4_acc_2: 0.3762 - dense_4_acc_3: 0.1105 - dense_4_acc_4: 0.9081 - dense_4_acc_5: 0.4924 - dense_4_acc_6: 0.1262 - dense_4_acc_7: 0.9279 - dense_4_acc_8: 0.3121 - dense_4_acc_9: 0.1221\n",
      "Epoch 2/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 7.8082 - dense_4_loss: 2.0215 - dense_4_acc: 0.9663 - dense_4_acc_1: 0.9677 - dense_4_acc_2: 0.6545 - dense_4_acc_3: 0.2833 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9351 - dense_4_acc_6: 0.4105 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.5982 - dense_4_acc_9: 0.2594\n",
      "Epoch 3/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 5.9382 - dense_4_loss: 1.7036 - dense_4_acc: 0.9744 - dense_4_acc_1: 0.9745 - dense_4_acc_2: 0.7698 - dense_4_acc_3: 0.4853 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9625 - dense_4_acc_6: 0.5963 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.6967 - dense_4_acc_9: 0.3644\n",
      "Epoch 4/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 4.4894 - dense_4_loss: 1.3981 - dense_4_acc: 0.9781 - dense_4_acc_1: 0.9775 - dense_4_acc_2: 0.8343 - dense_4_acc_3: 0.6905 - dense_4_acc_4: 0.9999 - dense_4_acc_5: 0.9687 - dense_4_acc_6: 0.7285 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.7643 - dense_4_acc_9: 0.4692\n",
      "Epoch 5/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 3.4699 - dense_4_loss: 1.0683 - dense_4_acc: 0.9792 - dense_4_acc_1: 0.9810 - dense_4_acc_2: 0.8563 - dense_4_acc_3: 0.8178 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9733 - dense_4_acc_6: 0.8033 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.8123 - dense_4_acc_9: 0.5965\n",
      "Epoch 6/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 2.7992 - dense_4_loss: 0.8322 - dense_4_acc: 0.9813 - dense_4_acc_1: 0.9826 - dense_4_acc_2: 0.8648 - dense_4_acc_3: 0.8819 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9754 - dense_4_acc_6: 0.8423 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.8450 - dense_4_acc_9: 0.6979\n",
      "Epoch 7/55\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 2.3472 - dense_4_loss: 0.6741 - dense_4_acc: 0.9828 - dense_4_acc_1: 0.9835 - dense_4_acc_2: 0.8720 - dense_4_acc_3: 0.9192 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9776 - dense_4_acc_6: 0.8682 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.8585 - dense_4_acc_9: 0.7609\n",
      "Epoch 8/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 2.0535 - dense_4_loss: 0.5690 - dense_4_acc: 0.9835 - dense_4_acc_1: 0.9845 - dense_4_acc_2: 0.8745 - dense_4_acc_3: 0.9349 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9800 - dense_4_acc_6: 0.8875 - dense_4_acc_7: 0.9999 - dense_4_acc_8: 0.8696 - dense_4_acc_9: 0.8023\n",
      "Epoch 9/55\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 1.7920 - dense_4_loss: 0.4701 - dense_4_acc: 0.9838 - dense_4_acc_1: 0.9852 - dense_4_acc_2: 0.8742 - dense_4_acc_3: 0.9443 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9801 - dense_4_acc_6: 0.9000 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.8824 - dense_4_acc_9: 0.8392\n",
      "Epoch 10/55\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 1.5993 - dense_4_loss: 0.4098 - dense_4_acc: 0.9856 - dense_4_acc_1: 0.9872 - dense_4_acc_2: 0.8772 - dense_4_acc_3: 0.9543 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9824 - dense_4_acc_6: 0.9109 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.8970 - dense_4_acc_9: 0.8602\n",
      "Epoch 11/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 1.4447 - dense_4_loss: 0.3626 - dense_4_acc: 0.9876 - dense_4_acc_1: 0.9891 - dense_4_acc_2: 0.8788 - dense_4_acc_3: 0.9548 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9831 - dense_4_acc_6: 0.9176 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9079 - dense_4_acc_9: 0.8783\n",
      "Epoch 12/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 1.3182 - dense_4_loss: 0.3247 - dense_4_acc: 0.9893 - dense_4_acc_1: 0.9904 - dense_4_acc_2: 0.8810 - dense_4_acc_3: 0.9594 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9839 - dense_4_acc_6: 0.9252 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9182 - dense_4_acc_9: 0.8924\n",
      "Epoch 13/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 1.2216 - dense_4_loss: 0.3054 - dense_4_acc: 0.9922 - dense_4_acc_1: 0.9916 - dense_4_acc_2: 0.8835 - dense_4_acc_3: 0.9612 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9856 - dense_4_acc_6: 0.9284 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9265 - dense_4_acc_9: 0.8990\n",
      "Epoch 14/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 1.1287 - dense_4_loss: 0.2729 - dense_4_acc: 0.9919 - dense_4_acc_1: 0.9924 - dense_4_acc_2: 0.8865 - dense_4_acc_3: 0.9644 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9845 - dense_4_acc_6: 0.9356 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9386 - dense_4_acc_9: 0.9128\n",
      "Epoch 15/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 1.0306 - dense_4_loss: 0.2457 - dense_4_acc: 0.9936 - dense_4_acc_1: 0.9934 - dense_4_acc_2: 0.8878 - dense_4_acc_3: 0.9700 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9867 - dense_4_acc_6: 0.9392 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9484 - dense_4_acc_9: 0.9262\n",
      "Epoch 16/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.9724 - dense_4_loss: 0.2301 - dense_4_acc: 0.9941 - dense_4_acc_1: 0.9938 - dense_4_acc_2: 0.8919 - dense_4_acc_3: 0.9692 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9876 - dense_4_acc_6: 0.9410 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9544 - dense_4_acc_9: 0.9298\n",
      "Epoch 17/55\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 0.9008 - dense_4_loss: 0.2103 - dense_4_acc: 0.9951 - dense_4_acc_1: 0.9940 - dense_4_acc_2: 0.8975 - dense_4_acc_3: 0.9748 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9882 - dense_4_acc_6: 0.9433 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9616 - dense_4_acc_9: 0.9394\n",
      "Epoch 18/55\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 0.8548 - dense_4_loss: 0.1955 - dense_4_acc: 0.9956 - dense_4_acc_1: 0.9953 - dense_4_acc_2: 0.9013 - dense_4_acc_3: 0.9754 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9889 - dense_4_acc_6: 0.9461 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9665 - dense_4_acc_9: 0.9481\n",
      "Epoch 19/55\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 0.7967 - dense_4_loss: 0.1787 - dense_4_acc: 0.9956 - dense_4_acc_1: 0.9954 - dense_4_acc_2: 0.9077 - dense_4_acc_3: 0.9758 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9891 - dense_4_acc_6: 0.9488 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9729 - dense_4_acc_9: 0.9526\n",
      "Epoch 20/55\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 0.7550 - dense_4_loss: 0.1679 - dense_4_acc: 0.9963 - dense_4_acc_1: 0.9963 - dense_4_acc_2: 0.9163 - dense_4_acc_3: 0.9771 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9897 - dense_4_acc_6: 0.9488 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9749 - dense_4_acc_9: 0.9558\n",
      "Epoch 21/55\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 0.7067 - dense_4_loss: 0.1548 - dense_4_acc: 0.9965 - dense_4_acc_1: 0.9965 - dense_4_acc_2: 0.9226 - dense_4_acc_3: 0.9810 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9907 - dense_4_acc_6: 0.9536 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9788 - dense_4_acc_9: 0.9607\n",
      "Epoch 22/55\n",
      "10000/10000 [==============================] - 38s 4ms/step - loss: 0.6716 - dense_4_loss: 0.1455 - dense_4_acc: 0.9966 - dense_4_acc_1: 0.9964 - dense_4_acc_2: 0.9314 - dense_4_acc_3: 0.9804 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9906 - dense_4_acc_6: 0.9525 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9776 - dense_4_acc_9: 0.9639\n",
      "Epoch 23/55\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 0.6394 - dense_4_loss: 0.1388 - dense_4_acc: 0.9973 - dense_4_acc_1: 0.9975 - dense_4_acc_2: 0.9395 - dense_4_acc_3: 0.9835 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9911 - dense_4_acc_6: 0.9550 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9802 - dense_4_acc_9: 0.9645\n",
      "Epoch 24/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.6000 - dense_4_loss: 0.1296 - dense_4_acc: 0.9975 - dense_4_acc_1: 0.9972 - dense_4_acc_2: 0.9477 - dense_4_acc_3: 0.9848 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9918 - dense_4_acc_6: 0.9563 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9836 - dense_4_acc_9: 0.9670\n",
      "Epoch 25/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.5656 - dense_4_loss: 0.1228 - dense_4_acc: 0.9976 - dense_4_acc_1: 0.9977 - dense_4_acc_2: 0.9543 - dense_4_acc_3: 0.9856 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9907 - dense_4_acc_6: 0.9586 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9842 - dense_4_acc_9: 0.9684\n",
      "Epoch 26/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.5309 - dense_4_loss: 0.1134 - dense_4_acc: 0.9976 - dense_4_acc_1: 0.9974 - dense_4_acc_2: 0.9625 - dense_4_acc_3: 0.9885 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9919 - dense_4_acc_6: 0.9592 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9857 - dense_4_acc_9: 0.9723\n",
      "Epoch 27/55\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 0.5064 - dense_4_loss: 0.1099 - dense_4_acc: 0.9985 - dense_4_acc_1: 0.9982 - dense_4_acc_2: 0.9699 - dense_4_acc_3: 0.9880 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9922 - dense_4_acc_6: 0.9605 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9868 - dense_4_acc_9: 0.9723\n",
      "Epoch 28/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.4808 - dense_4_loss: 0.1042 - dense_4_acc: 0.9986 - dense_4_acc_1: 0.9985 - dense_4_acc_2: 0.9741 - dense_4_acc_3: 0.9909 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9919 - dense_4_acc_6: 0.9603 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9880 - dense_4_acc_9: 0.9743\n",
      "Epoch 29/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.4560 - dense_4_loss: 0.1012 - dense_4_acc: 0.9990 - dense_4_acc_1: 0.9987 - dense_4_acc_2: 0.9777 - dense_4_acc_3: 0.9905 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9929 - dense_4_acc_6: 0.9627 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9898 - dense_4_acc_9: 0.9744\n",
      "Epoch 30/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.4361 - dense_4_loss: 0.0969 - dense_4_acc: 0.9986 - dense_4_acc_1: 0.9988 - dense_4_acc_2: 0.9826 - dense_4_acc_3: 0.9934 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9926 - dense_4_acc_6: 0.9604 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9885 - dense_4_acc_9: 0.9756\n",
      "Epoch 31/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.4120 - dense_4_loss: 0.0921 - dense_4_acc: 0.9991 - dense_4_acc_1: 0.9992 - dense_4_acc_2: 0.9842 - dense_4_acc_3: 0.9936 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9935 - dense_4_acc_6: 0.9627 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9891 - dense_4_acc_9: 0.9770\n",
      "Epoch 32/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.3965 - dense_4_loss: 0.0884 - dense_4_acc: 0.9991 - dense_4_acc_1: 0.9988 - dense_4_acc_2: 0.9868 - dense_4_acc_3: 0.9946 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9934 - dense_4_acc_6: 0.9641 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9903 - dense_4_acc_9: 0.9784\n",
      "Epoch 33/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.3771 - dense_4_loss: 0.0851 - dense_4_acc: 0.9996 - dense_4_acc_1: 0.9998 - dense_4_acc_2: 0.9894 - dense_4_acc_3: 0.9946 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9939 - dense_4_acc_6: 0.9643 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9912 - dense_4_acc_9: 0.9776\n",
      "Epoch 34/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.3635 - dense_4_loss: 0.0838 - dense_4_acc: 0.9998 - dense_4_acc_1: 0.9995 - dense_4_acc_2: 0.9899 - dense_4_acc_3: 0.9953 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9945 - dense_4_acc_6: 0.9665 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9906 - dense_4_acc_9: 0.9800\n",
      "Epoch 35/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.3526 - dense_4_loss: 0.0795 - dense_4_acc: 0.9992 - dense_4_acc_1: 0.9996 - dense_4_acc_2: 0.9901 - dense_4_acc_3: 0.9948 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9940 - dense_4_acc_6: 0.9663 - dense_4_acc_7: 0.9999 - dense_4_acc_8: 0.9917 - dense_4_acc_9: 0.9799\n",
      "Epoch 36/55\n",
      "10000/10000 [==============================] - 35s 3ms/step - loss: 0.3388 - dense_4_loss: 0.0762 - dense_4_acc: 0.9995 - dense_4_acc_1: 0.9993 - dense_4_acc_2: 0.9916 - dense_4_acc_3: 0.9950 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9941 - dense_4_acc_6: 0.9660 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9925 - dense_4_acc_9: 0.9815\n",
      "Epoch 37/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.3185 - dense_4_loss: 0.0726 - dense_4_acc: 0.9999 - dense_4_acc_1: 0.9996 - dense_4_acc_2: 0.9931 - dense_4_acc_3: 0.9959 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9942 - dense_4_acc_6: 0.9694 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9928 - dense_4_acc_9: 0.9826\n",
      "Epoch 38/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.3083 - dense_4_loss: 0.0704 - dense_4_acc: 0.9999 - dense_4_acc_1: 0.9999 - dense_4_acc_2: 0.9935 - dense_4_acc_3: 0.9963 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9946 - dense_4_acc_6: 0.9691 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9935 - dense_4_acc_9: 0.9839\n",
      "Epoch 39/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.2997 - dense_4_loss: 0.0686 - dense_4_acc: 0.9998 - dense_4_acc_1: 0.9999 - dense_4_acc_2: 0.9945 - dense_4_acc_3: 0.9961 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9946 - dense_4_acc_6: 0.9694 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9936 - dense_4_acc_9: 0.9837\n",
      "Epoch 40/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.2895 - dense_4_loss: 0.0657 - dense_4_acc: 0.9999 - dense_4_acc_1: 0.9999 - dense_4_acc_2: 0.9951 - dense_4_acc_3: 0.9961 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9945 - dense_4_acc_6: 0.9704 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9934 - dense_4_acc_9: 0.9837\n",
      "Epoch 41/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.2779 - dense_4_loss: 0.0639 - dense_4_acc: 0.9999 - dense_4_acc_1: 0.9999 - dense_4_acc_2: 0.9956 - dense_4_acc_3: 0.9965 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9950 - dense_4_acc_6: 0.9712 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9940 - dense_4_acc_9: 0.9842\n",
      "Epoch 42/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.2729 - dense_4_loss: 0.0635 - dense_4_acc: 0.9999 - dense_4_acc_1: 0.9999 - dense_4_acc_2: 0.9958 - dense_4_acc_3: 0.9966 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9952 - dense_4_acc_6: 0.9708 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9942 - dense_4_acc_9: 0.9852\n",
      "Epoch 43/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.2634 - dense_4_loss: 0.0600 - dense_4_acc: 0.9999 - dense_4_acc_1: 1.0000 - dense_4_acc_2: 0.9964 - dense_4_acc_3: 0.9967 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9952 - dense_4_acc_6: 0.9710 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9935 - dense_4_acc_9: 0.9856\n",
      "Epoch 44/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.2536 - dense_4_loss: 0.0583 - dense_4_acc: 1.0000 - dense_4_acc_1: 0.9998 - dense_4_acc_2: 0.9970 - dense_4_acc_3: 0.9967 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9953 - dense_4_acc_6: 0.9720 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9951 - dense_4_acc_9: 0.9854\n",
      "Epoch 45/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.2482 - dense_4_loss: 0.0563 - dense_4_acc: 1.0000 - dense_4_acc_1: 1.0000 - dense_4_acc_2: 0.9972 - dense_4_acc_3: 0.9968 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9952 - dense_4_acc_6: 0.9733 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9952 - dense_4_acc_9: 0.9868\n",
      "Epoch 46/55\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.2385 - dense_4_loss: 0.0546 - dense_4_acc: 0.9999 - dense_4_acc_1: 1.0000 - dense_4_acc_2: 0.9970 - dense_4_acc_3: 0.9967 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9952 - dense_4_acc_6: 0.9734 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9953 - dense_4_acc_9: 0.9884\n",
      "Epoch 47/55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 32s 3ms/step - loss: 0.2339 - dense_4_loss: 0.0533 - dense_4_acc: 0.9999 - dense_4_acc_1: 1.0000 - dense_4_acc_2: 0.9979 - dense_4_acc_3: 0.9970 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9956 - dense_4_acc_6: 0.9742 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9956 - dense_4_acc_9: 0.9870\n",
      "Epoch 48/55\n",
      "10000/10000 [==============================] - 32s 3ms/step - loss: 0.2287 - dense_4_loss: 0.0516 - dense_4_acc: 0.9999 - dense_4_acc_1: 0.9998 - dense_4_acc_2: 0.9977 - dense_4_acc_3: 0.9968 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9954 - dense_4_acc_6: 0.9744 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9954 - dense_4_acc_9: 0.9875\n",
      "Epoch 49/55\n",
      "10000/10000 [==============================] - 32s 3ms/step - loss: 0.2216 - dense_4_loss: 0.0508 - dense_4_acc: 0.9999 - dense_4_acc_1: 1.0000 - dense_4_acc_2: 0.9982 - dense_4_acc_3: 0.9969 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9966 - dense_4_acc_6: 0.9764 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9958 - dense_4_acc_9: 0.9879\n",
      "Epoch 50/55\n",
      "10000/10000 [==============================] - 32s 3ms/step - loss: 0.2162 - dense_4_loss: 0.0486 - dense_4_acc: 0.9999 - dense_4_acc_1: 1.0000 - dense_4_acc_2: 0.9984 - dense_4_acc_3: 0.9969 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9956 - dense_4_acc_6: 0.9766 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9960 - dense_4_acc_9: 0.9889\n",
      "Epoch 51/55\n",
      "10000/10000 [==============================] - 32s 3ms/step - loss: 0.2098 - dense_4_loss: 0.0479 - dense_4_acc: 0.9999 - dense_4_acc_1: 1.0000 - dense_4_acc_2: 0.9981 - dense_4_acc_3: 0.9969 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9963 - dense_4_acc_6: 0.9767 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9965 - dense_4_acc_9: 0.9889\n",
      "Epoch 52/55\n",
      "10000/10000 [==============================] - 32s 3ms/step - loss: 0.2045 - dense_4_loss: 0.0463 - dense_4_acc: 1.0000 - dense_4_acc_1: 1.0000 - dense_4_acc_2: 0.9984 - dense_4_acc_3: 0.9967 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9961 - dense_4_acc_6: 0.9773 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9965 - dense_4_acc_9: 0.9899\n",
      "Epoch 53/55\n",
      "10000/10000 [==============================] - 32s 3ms/step - loss: 0.1990 - dense_4_loss: 0.0448 - dense_4_acc: 0.9999 - dense_4_acc_1: 1.0000 - dense_4_acc_2: 0.9987 - dense_4_acc_3: 0.9970 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9962 - dense_4_acc_6: 0.9775 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9963 - dense_4_acc_9: 0.9901\n",
      "Epoch 54/55\n",
      "10000/10000 [==============================] - 32s 3ms/step - loss: 0.1927 - dense_4_loss: 0.0439 - dense_4_acc: 1.0000 - dense_4_acc_1: 1.0000 - dense_4_acc_2: 0.9988 - dense_4_acc_3: 0.9970 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9965 - dense_4_acc_6: 0.9781 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9965 - dense_4_acc_9: 0.9895\n",
      "Epoch 55/55\n",
      "10000/10000 [==============================] - 32s 3ms/step - loss: 0.1905 - dense_4_loss: 0.0426 - dense_4_acc: 1.0000 - dense_4_acc_1: 1.0000 - dense_4_acc_2: 0.9988 - dense_4_acc_3: 0.9970 - dense_4_acc_4: 1.0000 - dense_4_acc_5: 0.9961 - dense_4_acc_6: 0.9788 - dense_4_acc_7: 1.0000 - dense_4_acc_8: 0.9967 - dense_4_acc_9: 0.9903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efbb5845b70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = list(Yoh.swapaxes(0,1))\n",
    "model.fit([Xoh, s0, c0], outputs, epochs=55, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-03\n",
      "source: 5 April 09\n",
      "output: 2009-04-05\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-21\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09\n",
      "source: March 3 2001\n",
      "output: 2001-03-03\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "for example in EXAMPLES:\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "    source = np.array([source])\n",
    "    # print(source.shape)\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x612 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"8 Aug 2010\", num = 6, n_s = 128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
